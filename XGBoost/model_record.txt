# -*- coding: utf-8 -*-
"""
Created on Tue Mar 15 23:01:51 2022

@author: weber
"""
--------------------Previous data structure---------------
###General model(most used one)
xgboostModel = XGBClassifier(
    learning_rate=0.05, 
    n_estimators=400, 
    max_depth=2,
    tree_method='gpu_hist',
    min_child_weight=10, 
    objective='binary:logistic', 
    gamma=0.8,
    subsample=0.8,
    colsample_bytree=0.8,
    #early_stopping_rounds=100,
    eval_metric='error'
    )

With delay difference
### Don't want high number of estimators
xgboostModel = XGBClassifier(
    learning_rate=0.01, 
    n_estimators=400, 
    max_depth=2,
    tree_method='gpu_hist',
    min_child_weight=10, 
    objective='binary:logistic', 
    gamma=0.4,
    subsample=0.8,
    colsample_bytree=0.6,
    #early_stopping_rounds=100,
    eval_metric='error'
    )

### The best result (Tune second time)
xgboostModel = XGBClassifier(
    learning_rate=0.01, 
    n_estimators=500, 
    max_depth=8,
    tree_method='gpu_hist',
    min_child_weight=10, 
    objective='binary:logistic', 
    gamma=0.6,
    subsample=0.8,
    colsample_bytree=0.8,
    #early_stopping_rounds=100,
    eval_metric='error'
    )
xgboostModel = XGBClassifier(
    learning_rate=0.1, 
    n_estimators=100, 
    max_depth=6,
    tree_method='gpu_hist',
    min_child_weight=30, 
    objective='binary:logistic', 
    gamma=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    #early_stopping_rounds=100,
    eval_metric='error'
    )
xgboostModel = XGBClassifier(
    learning_rate=0.01, 
    n_estimators=1000, 
    max_depth=2,
    tree_method='gpu_hist',
    min_child_weight=10, 
    objective='binary:logistic', 
    gamma=0.4,
    subsample=0.8,
    colsample_bytree=0.6,
    #early_stopping_rounds=100,
    eval_metric='error'
    )

Without delay diff
### Do not wnat so extreme parameter
xgboostModel = XGBClassifier(
    learning_rate=0.1, 
    n_estimators=500, 
    max_depth=5,
    tree_method='gpu_hist',
    min_child_weight=10, 
    objective='binary:logistic', 
    gamma=0.8,
    subsample=0.6,
    colsample_bytree=0.6,
    #early_stopping_rounds=100,
    eval_metric='error'
    )
    
### The best result
xgboostModel = XGBClassifier(
    learning_rate=0.1, 
    n_estimators=1000, 
    max_depth=10,
    tree_method='gpu_hist',
    min_child_weight=10, 
    objective='binary:logistic', 
    gamma=0.8,
    subsample=0.6,
    colsample_bytree=0.6,
    #early_stopping_rounds=100,
    eval_metric='error'
    )
    
----------Parity vector only-----------------------------------------
With parity vector no delay diff
### Normal one with not too much max depth and have 87%
xgboostModel = XGBClassifier(
    booster='gbtree', colsample_bytree=0.6,
              eval_metric='error', gamma=0.6,
              learning_rate=0.1, max_depth=4,
              min_child_weight=10, n_estimators=500, subsample=0.8, tree_method='gpu_hist'
    )
    
### The best result
xgboostModel = XGBClassifier(
    booster='gbtree', colsample_bytree=0.8,
              eval_metric='error', gamma=0.4,
              learning_rate=0.1, max_depth=6,
              min_child_weight=20, n_estimators=1000, subsample=0.6, tree_method='gpu_hist'
    )

With parity vector and delay diff    
### Add in delay diff but not too much depth
xgboostModel = XGBClassifier(
    booster='gbtree', colsample_bytree=0.8,
              eval_metric='error', gamma=0.6,
              learning_rate=0.1, max_depth=4,
              min_child_weight=20, n_estimators=500, subsample=0.6, tree_method='gpu_hist'
    )


### The best result
xgboostModel = XGBClassifier(
    booster='gbtree', colsample_bytree=0.8,
              eval_metric='error', gamma=0.6,
              learning_rate=0.1, max_depth=6,
              min_child_weight=20, n_estimators=1000, subsample=0.6, tree_method='gpu_hist'
    )
    
------------Only delay difference
### Not using those extreme parameters
xgboostModel = XGBClassifier(
    learning_rate=0.1, 
    n_estimators=500, 
    max_depth=4,
    tree_method='gpu_hist',
    min_child_weight=40, 
    objective='binary:logistic', 
    gamma=0.1,
    subsample=0.1,
    colsample_bytree=0.8,
    #early_stopping_rounds=100,
    eval_metric='error'
    )

### The best result
xgboostModel = XGBClassifier(
    learning_rate=0.1, 
    n_estimators=1000, 
    max_depth=8,
    tree_method='gpu_hist',
    min_child_weight=40, 
    objective='binary:logistic', 
    gamma=0.1,
    subsample=0.1,
    colsample_bytree=0.8,
    #early_stopping_rounds=100,
    eval_metric='error'
    )